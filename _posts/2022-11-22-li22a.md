---
title: 'OSLAT: Open Set Label Attention Transformer for Medical Entity Retrieval and
  Span Extraction'
software: https://github.com/curai/curai-research/tree/main/OSLAT
abstract: 'Medical entity span extraction and linking are critical steps for many
  healthcare NLP tasks. Most existing entity extraction methods either have a fixed
  vocabulary of medical entities or require span annotations. In this paper, we propose
  a method for linking an open set of entities that does not require any span annotations.
  Our method, <b>Open Set Label Attention Transformer (OSLAT)</b>, uses the label-attention
  mechanism to learn candidate-entity contextualized text representations. We find
  that OSLAT can not only link entities but is also able to implicitly learn spans
  associated with entities. We evaluate OSLAT on two tasks: (1) span extraction trained
  without explicit span annotations, and (2) entity linking trained without span-level
  annotation. We test the generalizability of our method by training two separate
  models on two datasets with low entity overlap and comparing cross-dataset performance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22a
month: 0
tex_title: 'OSLAT: Open Set Label Attention Transformer for Medical Entity Retrieval
  and Span Extraction'
firstpage: 373
lastpage: 390
page: 373-390
order: 373
cycles: false
bibtex_author: Li, Raymond and Valmianski, Ilya and Deng, Li and Amatriain, Xavier
  and Kannan, Anitha
author:
- given: Raymond
  family: Li
- given: Ilya
  family: Valmianski
- given: Li
  family: Deng
- given: Xavier
  family: Amatriain
- given: Anitha
  family: Kannan
date: 2022-11-22
address:
container-title: Proceedings of the 2nd Machine Learning for Health symposium
volume: '193'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 11
  - 22
pdf: https://proceedings.mlr.press/v193/li22a/li22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
